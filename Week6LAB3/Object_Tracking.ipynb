{"cells":[{"cell_type":"markdown","id":"59f0fd61","metadata":{"id":"59f0fd61"},"source":["# Object Detection and Tracking Tutorial\n","In this tutorial, we will perform object detection and tracking using a video file. We will annotate the detected objects and save the output as a labeled video.\n","## Steps Involved:\n","1. Importing important packages\n","2. Displaying the video\n","3. Reading metadata from the video\n","4. Extracting images from the video\n","5. Annotating frames with labels\n","6. Saving the annotated video"]},{"cell_type":"markdown","id":"4a876132","metadata":{"id":"4a876132"},"source":["### Important Note ⚠️\n","**Do not change the name of the video file**. The labels are extracted from a CSV file that contains annotations for multiple videos, and the video name is used to match the labels with the correct video. Make sure to use the video file with its original name (e.g., `026c7465-309f6d33.mp4`)."]},{"cell_type":"markdown","id":"6bbd99b6","metadata":{"id":"6bbd99b6"},"source":["### Step 1: Import Important Packages\n","We will import the necessary packages such as Pandas, Numpy, OpenCV, and Matplotlib. We will also use IPython for displaying the video within the notebook."]},{"cell_type":"code","execution_count":null,"id":"e5dacf22","metadata":{"id":"e5dacf22"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","from glob import glob\n","import IPython.display as ipd\n","from tqdm import tqdm\n","import subprocess\n","plt.style.use('ggplot')"]},{"cell_type":"markdown","id":"02cff852","metadata":{"id":"02cff852"},"source":["### Step 2: Display Video\n","To display the video, we use IPython's display functionality."]},{"cell_type":"code","execution_count":null,"id":"14598cf6","metadata":{"id":"14598cf6"},"outputs":[],"source":["ipd.Video('026c7465-309f6d33.mp4', width=700)"]},{"cell_type":"markdown","id":"8b0e82c9","metadata":{"id":"8b0e82c9"},"source":["### Step 3: Read Video Metadata\n","We will open the video file using OpenCV, and retrieve its properties like total number of frames, height, width, and frames per second (FPS)."]},{"cell_type":"code","execution_count":null,"id":"4991e2d9","metadata":{"id":"4991e2d9"},"outputs":[],"source":["cap = cv2.VideoCapture('026c7465-309f6d33.mp4')\n","# Get total number of frames\n","frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n","# Get video height and width\n","height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n","# Get frames per second\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","cap.release()\n","print(f'Height: {height}, Width: {width}, FPS: {fps}')"]},{"cell_type":"markdown","id":"6183a326","metadata":{"id":"6183a326"},"source":["### Step 4: Extract and Display Video Frames\n","We will now extract individual frames from the video, display them using Matplotlib, and annotate the frames with bounding boxes."]},{"cell_type":"code","execution_count":null,"id":"f48941e2","metadata":{"id":"f48941e2"},"outputs":[],"source":["# Helper function to display OpenCV images in a notebook\n","def display_cv2_img(img, figsize=(10, 10)):\n","    img_ = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    fig, ax = plt.subplots(figsize=figsize)\n","    ax.imshow(img_)\n","    ax.axis('off')\n","\n","# Extract and display a frame\n","cap = cv2.VideoCapture('026c7465-309f6d33.mp4')\n","ret, img = cap.read()\n","cap.release()\n","if ret:\n","    display_cv2_img(img)"]},{"cell_type":"markdown","id":"ec0fa5c4","metadata":{"id":"ec0fa5c4"},"source":["### Step 5: Annotate Frames with Labels\n","We will annotate the detected objects with bounding boxes using OpenCV, and add category labels to the video frames."]},{"cell_type":"code","execution_count":null,"id":"2b9b0dc8","metadata":{"id":"2b9b0dc8"},"outputs":[],"source":["# Read the labels from CSV\n","labels = pd.read_csv('labels.csv', low_memory=False)\n","video_labels = labels.query('videoName == \"026c7465-309f6d33\"').reset_index(drop=True)\n","video_labels['video_frame'] = (video_labels['frameIndex'] * 11.9).round().astype(int)\n"]},{"cell_type":"code","source":["video_labels[\"category\"].value_counts()"],"metadata":{"id":"TH15vbgidkG3"},"id":"TH15vbgidkG3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will annotate and show frame number 1035 (Selected Randomly)"],"metadata":{"id":"Jkl0E4UZdu_-"},"id":"Jkl0E4UZdu_-"},{"cell_type":"code","source":["\n","cap = cv2.VideoCapture('026c7465-309f6d33.mp4')\n","for frame in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n","    ret, img = cap.read()\n","    if frame == 1035:\n","        break\n","cap.release()\n","# Annotate the frame\n","frame_labels = video_labels.query('video_frame == 1035')\n","for _, d in frame_labels.iterrows():\n","    pt1 = int(d['box2d.x1']), int(d['box2d.y1'])\n","    pt2 = int(d['box2d.x2']), int(d['box2d.y2'])\n","    cv2.rectangle(img, pt1, pt2, (0, 0, 255), 3)\n","display_cv2_img(img)"],"metadata":{"id":"drOqq5nfdnfy"},"id":"drOqq5nfdnfy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Step 6: Displaying Objects Colored by Category"],"metadata":{"id":"swMzULW0eFS1"},"id":"swMzULW0eFS1"},{"cell_type":"code","source":["color_map = {\n","    \"car\": (0, 0, 255),\n","    \"truck\": (0, 0, 100),\n","    \"pedestrian\": (255, 0, 0),\n","    \"other vehicle\": (0, 0, 150),\n","    \"rider\": (200, 0, 0),\n","    \"bicycle\": (0, 255, 0),\n","    \"other person\": (200, 0, 0),\n","    \"trailer\": (0, 150, 150),\n","    \"motorcycle\": (0, 150, 0),\n","    \"bus\": (0, 0, 100),\n","}\n","\n","img_example = img.copy()\n","frame_labels = video_labels.query('video_frame == 1035')\n","for i, d in frame_labels.iterrows():\n","    pt1 = int(d['box2d.x1']), int(d['box2d.y1'])\n","    pt2 = int(d['box2d.x2']), int(d['box2d.y2'])\n","    color = color_map[d['category']]\n","    cv2.rectangle(img_example, pt1, pt2, color, 3)\n","\n","display_cv2_img(img_example)"],"metadata":{"id":"xPMHBGCOeScl"},"id":"xPMHBGCOeScl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Step 7: Adding Text"],"metadata":{"id":"Hd-v5i5Deek3"},"id":"Hd-v5i5Deek3"},{"cell_type":"code","source":["frame_labels = video_labels.query(\"video_frame == @frame\")\n","font = cv2.FONT_HERSHEY_TRIPLEX\n","img_example = img.copy()\n","for i, d in frame_labels.iterrows():\n","    pt1 = int(d[\"box2d.x1\"]), int(d[\"box2d.y1\"])\n","    pt2 = int(d[\"box2d.x2\"]), int(d[\"box2d.y2\"])\n","    color = color_map[d[\"category\"]]\n","    img_example = cv2.rectangle(img_example, pt1, pt2, color, 3)\n","    pt_text = int(d[\"box2d.x1\"]) + 5, int(d[\"box2d.y1\"] + 10)\n","    img_example = cv2.putText(img_example, d[\"category\"], pt_text, font, 0.5, color)\n","display_cv2_img(img_example)\n","cap.release()"],"metadata":{"id":"xoDbaPiIejCJ"},"id":"xoDbaPiIejCJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"46ad3deb","metadata":{"id":"46ad3deb"},"source":["### Step 8: Save Annotated Video\n","We will write the annotations back to the video and save the output using OpenCV's `VideoWriter`. Finally, we compress the video using `ffmpeg`."]},{"cell_type":"code","execution_count":null,"id":"e7ca495a","metadata":{"id":"e7ca495a"},"outputs":[],"source":["# Function to add annotations to video\n","def add_annotations(img, frame, video_labels):\n","    max_frame = video_labels.query('video_frame <= @frame')['video_frame'].max()\n","    frame_labels = video_labels.query('video_frame == @max_frame')\n","    for _, d in frame_labels.iterrows():\n","        pt1 = int(d['box2d.x1']), int(d['box2d.y1'])\n","        pt2 = int(d['box2d.x2']), int(d['box2d.y2'])\n","        color = color_map[d[\"category\"]]\n","        img = cv2.rectangle(img, pt1, pt2, color, 3)\n","    return img\n","\n","# Writing annotations to video\n","out = cv2.VideoWriter('out_test.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 59.94, (1280, 720))\n","cap = cv2.VideoCapture('026c7465-309f6d33.mp4')\n","for frame in tqdm(range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))):\n","    ret, img = cap.read()\n","    if not ret:\n","        break\n","    img = add_annotations(img, frame, video_labels)\n","    out.write(img)\n","cap.release()\n","out.release()"]},{"cell_type":"markdown","id":"813bb64e","metadata":{"id":"813bb64e"},"source":["## Conclusion\n","In this tutorial, we performed object detection and tracking using OpenCV and YOLO. We annotated the objects and saved the results in a video file.\n","[Source](https://www.kaggle.com/code/kirollosashraf/driving-video-object-tracking/notebook#Step1%7C-Import-important-packages) for this tutorial.\n"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}